{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18846\n",
      "--------------------------------------------------\n",
      "From: Mamatha Devineni Ratnam <mr47+@andrew.cmu.edu>\n",
      "Subject: Pens fans reactions\n",
      "Organization: Post Office, Carnegie Mellon, Pittsburgh, PA\n",
      "Lines: 12\n",
      "NNTP-Posting-Host: po4.andrew.cmu.edu\n",
      "\n",
      "\n",
      "\n",
      "I am sure some bashers of Pens fans are pretty confused about the lack\n",
      "of any kind of posts about the recent Pens massacre of the Devils. Actually,\n",
      "I am  bit puzzled too and a bit relieved. However, I am going to put an end\n",
      "to non-PIttsburghers' relief with a bit of praise for the Pens. Man, they\n",
      "are killing those Devils worse than I thought. Jagr just showed you why\n",
      "he is much better than his regular season stats. He is also a lot\n",
      "fo fun to watch in the playoffs. Bowman should let JAgr have a lot of\n",
      "fun in the next couple of games since the Pens are going to beat the pulp out of Jersey anyway. I was very disappointed not to see the Islanders lose the final\n",
      "regular season game.          PENS RULE!!!\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "[10  3 17 ...  3  1  7]\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]\n",
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "朴素贝叶斯进行文本分类\n",
    ":return: None\n",
    "\"\"\"\n",
    "from sklearn.datasets import fetch_20newsgroups #新闻数据\n",
    "import numpy as np\n",
    "\n",
    "news = fetch_20newsgroups(subset='all', data_home='../MachineData/Data')\n",
    "\n",
    "print(len(news.data))  #样本数，包含的特征\n",
    "print('-'*50)\n",
    "print(news.data[0]) #第一个样本 特征\n",
    "print('-'*50)\n",
    "\n",
    "print(news.target) #18846个样本分别是什么标签\n",
    "print(np.unique(news.target))#对所有标签去重，一共有多少个标签，也就是多少个种类\n",
    "print(news.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "<class 'list'>\n",
      "--------------------------------------------------\n",
      "From: feustel@netcom.com (David Feustel)\n",
      "Subject: Re: BATF/FBI Murders Almost Everyone in Waco Today! 4/19\n",
      "Organization: DAFCO: OS/2 Software Support & Consulting\n",
      "Lines: 10\n",
      "\n",
      "It's truly unfortunate that we don't have the Japanese tradition of\n",
      "Hari-Kari for public officials to salvage some tatters of honor after\n",
      "they commit offenses against humanity like were perpetrated in Waco,\n",
      "Texas today.\n",
      "-- \n",
      "Dave Feustel N9MYI <feustel@netcom.com>\n",
      "\n",
      "I'm beginning to look forward to reaching the %100 allocation of taxes\n",
      "to pay for the interest on the national debt. At that point the\n",
      "federal government will be will go out of business for lack of funds.\n",
      "\n",
      "--------------------------------------------------\n",
      "From: donyee@athena.mit.edu (Donald Yee)\n",
      "Subject: S3 86c805 w/2MB = 1024x768x32k colors = Orchid Pipe Dream?\n",
      "Organization: Massachusetts Institute of Technology\n",
      "Lines: 36\n",
      "Distribution: world\n",
      "NNTP-Posting-Host: pesto.mit.edu\n",
      "\n",
      "Hi\n",
      "\tI have an Orchid Fahrenheit VLB with 2MB of DRAM.  It is an S3\n",
      "86c805 based card.  I had a problem for a while after installing my\n",
      "second meg of DRAM for the video, and thanks to Orchid, I got a fix\n",
      "from their tech support (it was jumper settings not given in the\n",
      "ordinary manual.  I assume it would come with memory ordered from\n",
      "them, so I guess I should be glad they didn't just say \"Buy the memory\n",
      "from us\" or something like that.)\n",
      "\n",
      "\tThe one thing that I was puzzled by was why there was not a\n",
      "1024x768x32k color mode on the thing, either in full screen or\n",
      "enlarged desktop mode.  My ATI Ultra Plus can handle that, given 2MB\n",
      "of memory.  All the 2MB buys you on the Fahrenheit is 1280x1024x256.\n",
      "Just ONE more mode.  GEEZ.  Had I known, I wouldn't have bothered.  I\n",
      "asked them why, and all I got was \"Your point is well taken, but\n",
      "Orchid's software developers are busy with other projects.\"\n",
      "\n",
      "\tSo, to get to the point, finally, ARE there any s3 86c805\n",
      "drivers out there that can handle high res hicolor modes?  I'd love to\n",
      "get another card, but perhaps it will have to wait until the next\n",
      "generation of cards comes out, since this card came bundled with my\n",
      "system and it's not so easy to exchange these things unless they're\n",
      "broken.\n",
      "\n",
      "\tIf you want these modes, steer away from Orchids s3 86c805\n",
      "cards (ie. VLB or VA/VLB), at least until their developers are \"less\n",
      "busy\".  If the magazines are to believed, I've only seen one s3 86c805\n",
      "product thus far which can handle 1024x768x32k color (Genoa?),\n",
      "although evenn that might be a misprint.\n",
      "\n",
      "\tPlease, if there are generic or semi-generic drivers out\n",
      "there, let me know where I can get them.  800x600x32k is OK, but I\n",
      "coulda gotten that with my ATI VGA Wonder XL.\n",
      "\n",
      "Thanks.\n",
      "donyee@athena.mit.edu\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "<class 'scipy.sparse._csr.csr_matrix'>\n",
      "(14134, 153196)\n",
      "--------------------------------------------------\n",
      "153196\n",
      "['00' '000' '0000' '00000' '0000000004' '0000000005' '0000000667'\n",
      " '0000001200' '000003' '000005102000']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split #数据分割\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer #特征抽取\n",
    "\n",
    "print('-'*50)\n",
    "# 进行数据分割\n",
    "x_train, x_test, y_train, y_test = train_test_split(news.data, news.target, test_size=0.25, random_state=1)\n",
    "\n",
    "\n",
    "print('-'*50)\n",
    "print(type(x_train))\n",
    "print('-'*50)\n",
    "print(x_train[0])\n",
    "print('-'*50)\n",
    "print(x_train[1])\n",
    "print('-'*50)\n",
    "\n",
    "# 对数据集进行特征抽取\n",
    "tf = TfidfVectorizer()\n",
    "print('-'*50)\n",
    "\n",
    "# 以训练集当中的词的列表进行每篇文章重要性统计['a','b','c','d']\n",
    "#x_train经过tf.fit transform之后，变成了一个稀疏矩阵\n",
    "#横坐标是文章的序号，纵坐标是词的序号，值是词的重要性\n",
    "x_train = tf.fit_transform(x_train)\n",
    "print(type(x_train))\n",
    "print(x_train.shape)\n",
    "print('-'*50)\n",
    "\n",
    "#针对特征内容，可以自行打印\n",
    "#tf.get_feature_names()是所有词（特征）的列表\n",
    "print(len(tf.get_feature_names_out()))\n",
    "print(tf.get_feature_names_out()[0:10])\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# 下面这段代码请注意，不要重复运行，第一次运行时，可以跑通，如果重复运行。\n",
    "# 会报如下错误：\n",
    "#  AttributeError: 'csr_matrix' object has no attribute 'lower'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'csr_matrix' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[6], line 5\u001B[0m\n\u001B[0;32m      3\u001B[0m start\u001B[38;5;241m=\u001B[39mtime\u001B[38;5;241m.\u001B[39mtime()\n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m#测试集的特征值经过transform之后，变成了一个稀疏矩阵\u001B[39;00m\n\u001B[1;32m----> 5\u001B[0m x_test \u001B[38;5;241m=\u001B[39m \u001B[43mtf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_test\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m#特征数目不发生改变\u001B[39;00m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28mlen\u001B[39m(tf\u001B[38;5;241m.\u001B[39mget_feature_names_out()))\n\u001B[0;32m      9\u001B[0m \u001B[38;5;66;03m# 进行朴素贝叶斯算法的预测,alpha是拉普拉斯平滑系数，分子和分母加上一个系数，分母加alpha*特征词数目\u001B[39;00m\n",
      "File \u001B[1;32mD:\\PY\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:2157\u001B[0m, in \u001B[0;36mTfidfVectorizer.transform\u001B[1;34m(self, raw_documents)\u001B[0m\n\u001B[0;32m   2140\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Transform documents to document-term matrix.\u001B[39;00m\n\u001B[0;32m   2141\u001B[0m \n\u001B[0;32m   2142\u001B[0m \u001B[38;5;124;03mUses the vocabulary and document frequencies (df) learned by fit (or\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   2153\u001B[0m \u001B[38;5;124;03m    Tf-idf-weighted document-term matrix.\u001B[39;00m\n\u001B[0;32m   2154\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   2155\u001B[0m check_is_fitted(\u001B[38;5;28mself\u001B[39m, msg\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe TF-IDF vectorizer is not fitted\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m-> 2157\u001B[0m X \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mraw_documents\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2158\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tfidf\u001B[38;5;241m.\u001B[39mtransform(X, copy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "File \u001B[1;32mD:\\PY\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1433\u001B[0m, in \u001B[0;36mCountVectorizer.transform\u001B[1;34m(self, raw_documents)\u001B[0m\n\u001B[0;32m   1430\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_vocabulary()\n\u001B[0;32m   1432\u001B[0m \u001B[38;5;66;03m# use the same matrix-building strategy as fit_transform\u001B[39;00m\n\u001B[1;32m-> 1433\u001B[0m _, X \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_count_vocab\u001B[49m\u001B[43m(\u001B[49m\u001B[43mraw_documents\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfixed_vocab\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m   1434\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbinary:\n\u001B[0;32m   1435\u001B[0m     X\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mfill(\u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[1;32mD:\\PY\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1275\u001B[0m, in \u001B[0;36mCountVectorizer._count_vocab\u001B[1;34m(self, raw_documents, fixed_vocab)\u001B[0m\n\u001B[0;32m   1273\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m doc \u001B[38;5;129;01min\u001B[39;00m raw_documents:\n\u001B[0;32m   1274\u001B[0m     feature_counter \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m-> 1275\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m feature \u001B[38;5;129;01min\u001B[39;00m \u001B[43manalyze\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdoc\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[0;32m   1276\u001B[0m         \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1277\u001B[0m             feature_idx \u001B[38;5;241m=\u001B[39m vocabulary[feature]\n",
      "File \u001B[1;32mD:\\PY\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:111\u001B[0m, in \u001B[0;36m_analyze\u001B[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001B[0m\n\u001B[0;32m    109\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    110\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m preprocessor \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 111\u001B[0m         doc \u001B[38;5;241m=\u001B[39m \u001B[43mpreprocessor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdoc\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    112\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m tokenizer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    113\u001B[0m         doc \u001B[38;5;241m=\u001B[39m tokenizer(doc)\n",
      "File \u001B[1;32mD:\\PY\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:69\u001B[0m, in \u001B[0;36m_preprocess\u001B[1;34m(doc, accent_function, lower)\u001B[0m\n\u001B[0;32m     50\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Chain together an optional series of text preprocessing steps to\u001B[39;00m\n\u001B[0;32m     51\u001B[0m \u001B[38;5;124;03mapply to a document.\u001B[39;00m\n\u001B[0;32m     52\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     66\u001B[0m \u001B[38;5;124;03m    preprocessed string\u001B[39;00m\n\u001B[0;32m     67\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     68\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m lower:\n\u001B[1;32m---> 69\u001B[0m     doc \u001B[38;5;241m=\u001B[39m \u001B[43mdoc\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlower\u001B[49m()\n\u001B[0;32m     70\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m accent_function \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m     71\u001B[0m     doc \u001B[38;5;241m=\u001B[39m accent_function(doc)\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'csr_matrix' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import time\n",
    "start=time.time()\n",
    "#测试集的特征值经过transform之后，变成了一个稀疏矩阵\n",
    "x_test = tf.transform(x_test)  #特征数目不发生改变\n",
    "print(len(tf.get_feature_names_out()))\n",
    "\n",
    "\n",
    "# 进行朴素贝叶斯算法的预测,alpha是拉普拉斯平滑系数，分子和分母加上一个系数，分母加alpha*特征词数目\n",
    "mlt = MultinomialNB(alpha=1.0)\n",
    "\n",
    "print(x_train.toarray())#稀疏矩阵转换为数组\n",
    "# 训练\n",
    "mlt.fit(x_train, y_train)#训练集的特征值和目标值\n",
    "\n",
    "end=time.time()\n",
    "end-start"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测的文章类别为： [16 19 18 ... 13  7 14]\n",
      "准确率为： 0.8518675721561969\n"
     ]
    },
    {
     "data": {
      "text/plain": "0.03490042686462402"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start=time.time()\n",
    "y_predict = mlt.predict(x_test) #y_predict是预测的目标值，拿x_test去预测(mlt,是训练好的模型)\n",
    "\n",
    "print(\"预测的文章类别为：\", y_predict)\n",
    "\n",
    "# 得出准确率,这个是很难提高准确率，为什么呢？\n",
    "print(\"准确率为：\", mlt.score(x_test, y_test))\n",
    "end=time.time()\n",
    "end-start"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "每个类别的精确率和召回率：                           precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.91      0.77      0.83       199\n",
      "           comp.graphics       0.83      0.79      0.81       242\n",
      " comp.os.ms-windows.misc       0.89      0.83      0.86       263\n",
      "comp.sys.ibm.pc.hardware       0.80      0.83      0.81       262\n",
      "   comp.sys.mac.hardware       0.90      0.88      0.89       234\n",
      "          comp.windows.x       0.92      0.85      0.88       230\n",
      "            misc.forsale       0.96      0.67      0.79       257\n",
      "               rec.autos       0.90      0.87      0.88       265\n",
      "         rec.motorcycles       0.90      0.95      0.92       251\n",
      "      rec.sport.baseball       0.89      0.96      0.93       226\n",
      "        rec.sport.hockey       0.95      0.98      0.96       262\n",
      "               sci.crypt       0.76      0.97      0.85       257\n",
      "         sci.electronics       0.84      0.80      0.82       229\n",
      "                 sci.med       0.97      0.86      0.91       249\n",
      "               sci.space       0.92      0.96      0.94       256\n",
      "  soc.religion.christian       0.55      0.98      0.70       243\n",
      "      talk.politics.guns       0.76      0.96      0.85       234\n",
      "   talk.politics.mideast       0.93      0.99      0.96       224\n",
      "      talk.politics.misc       0.98      0.56      0.72       197\n",
      "      talk.religion.misc       0.97      0.26      0.41       132\n",
      "\n",
      "                accuracy                           0.85      4712\n",
      "               macro avg       0.88      0.84      0.84      4712\n",
      "            weighted avg       0.87      0.85      0.85      4712\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report# 准确率，召回率，F1分数\n",
    "# 目前这个场景我们不需要召回率，support是真实的为那个类别的有多少个样本\n",
    "print(\"每个类别的精确率和召回率：\", classification_report(y_test, y_predict,\n",
    "                                             target_names=news.target_names))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "230\n",
      "214\n",
      "AUC指标： 0.924078924393225\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "# 把0-19总计20个分类，变为0和1\n",
    "# 5是可以改为0到19的\n",
    "y_test1 = np.where(y_test == 5, 1, 0)\n",
    "print(y_test1.sum())\n",
    "y_predict1 = np.where(y_predict == 5, 1, 0)\n",
    "print(y_predict1.sum())\n",
    "# roc_auc_score的y_test只能是二分类,针对多分类如何计算AUC\n",
    "print(\"AUC指标：\", roc_auc_score(y_test1, y_predict1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "del news\n",
    "del x_train\n",
    "del x_test\n",
    "del y_test\n",
    "del y_predict\n",
    "del tf\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}