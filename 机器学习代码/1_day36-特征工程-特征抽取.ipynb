{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer #字典数据抽取\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer#文本特征抽取\n",
    "from sklearn.preprocessing import MinMaxScaler# 归一化\n",
    "from sklearn.preprocessing import StandardScaler# 标准化\n",
    "from sklearn.feature_selection import VarianceThreshold#特征选择\n",
    "from sklearn.decomposition import PCA#主成分分析\n",
    "import jieba#中文分词\n",
    "import numpy as np#科学计算\n",
    "from sklearn.impute import SimpleImputer#缺失值处理"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1 数据中含有字符串的时候，如何做特征抽取"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t1.0\n",
      "  (0, 3)\t100.0\n",
      "  (1, 0)\t1.0\n",
      "  (1, 3)\t60.0\n",
      "  (2, 2)\t1.0\n",
      "  (2, 3)\t30.0\n",
      "--------------------------------------------------\n",
      "['city=上海' 'city=北京' 'city=深圳' 'temperature']\n",
      "--------------------------------------------------\n",
      "[{'city=北京': 1.0, 'temperature': 100.0}, {'city=上海': 1.0, 'temperature': 60.0}, {'city=深圳': 1.0, 'temperature': 30.0}]\n"
     ]
    }
   ],
   "source": [
    "def dict2vec(): #字典数据抽取\n",
    "#NO1 实例化，DictVectorizer()\n",
    "#NO2 sparse改为True,输出的是每个不为零位置的坐标\n",
    "# （矩阵中存在大量的0，sparse存储只记录非零位置，节省空间的作用），\n",
    "# 稀疏矩阵可以节省存储空间\n",
    "#NO3 给一个字典实例 #每个样本都是一个字典，有三个样本\n",
    "#NO4 调用fit_transform\n",
    "#NO5 打印结果\n",
    "# vectorizer中文含义是矢量器的含义\n",
    "\n",
    "    dict = DictVectorizer(sparse=True)  # 把sparse改为True看看\n",
    "\n",
    "\n",
    "    # 调用fit_transform\n",
    "    data = dict.fit_transform([{'city': '北京', 'temperature': 100},\n",
    "                               {'city': '上海', 'temperature': 60},\n",
    "                               {'city': '深圳', 'temperature': 30}])\n",
    "    print(data)\n",
    "    print('-' * 50)\n",
    "    # 字典中的一些类别数据，分别进行转换成特征\n",
    "    print(dict.get_feature_names_out())\n",
    "    print('-' * 50)\n",
    "    #去看每个特征代表的含义，逆转回去\n",
    "    print(dict.inverse_transform(data))\n",
    "    return None\n",
    "\n",
    "dict2vec()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2 一段英文文本如何变为数值类型"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dislike' 'is' 'life' 'like' 'long' 'python' 'short' 'too']\n",
      "--------------------------------------------------\n",
      "  (0, 2)\t2\n",
      "  (0, 1)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 5)\t1\n",
      "  (1, 2)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 5)\t1\n",
      "  (1, 7)\t1\n",
      "  (1, 4)\t1\n",
      "  (1, 0)\t1\n",
      "  (2, 2)\t1\n",
      "  (2, 1)\t1\n",
      "  (2, 6)\t1\n",
      "--------------------------------------------------\n",
      "<class 'scipy.sparse._csr.csr_matrix'>\n",
      "--------------------------------------------------\n",
      "[[0 1 2 1 0 1 1 0]\n",
      " [1 1 1 0 1 1 0 1]\n",
      " [0 1 1 0 0 0 1 0]]\n",
      "--------------------------------------------------\n",
      "[array(['life', 'is', 'short', 'like', 'python'], dtype='<U7'), array(['life', 'is', 'python', 'too', 'long', 'dislike'], dtype='<U7'), array(['life', 'is', 'short'], dtype='<U7')]\n"
     ]
    }
   ],
   "source": [
    "#text---->number(vector)\n",
    "def text2vec_en():\n",
    "\n",
    "#NO1\n",
    "# 实例化CountVectorizer\n",
    "# 默认会去除单个字母的单词，默认认为这个词对整个样本没有影响\n",
    "# max_df, min_df--整数：\n",
    "# 指每个词的所有文档词频数不小于min_df\n",
    "# 出现该词的文档数目小于等于max_df\n",
    "# max_df, min_df--小数：\n",
    "\n",
    "# 某个词的出现的次数／所有文档数量\n",
    "# min_df=2\n",
    "    vector = CountVectorizer(min_df=1)\n",
    "#NO2\n",
    "# 调用fit_transform输入并转换数据\n",
    "    test1=\"life is  short,i like python life\"\n",
    "    test2=\"life is too long,i dislike python\"\n",
    "    test3=\"life is short\"\n",
    "    res = vector.fit_transform([test1,test2,test3])\n",
    "\n",
    "    # 打印结果,把每个词都分离了\n",
    "    print(vector.get_feature_names_out())\n",
    "    print('-'*50)\n",
    "    print(res) #稀疏矩阵\n",
    "    print('-'*50)\n",
    "    print(type(res)) #<class 'scipy.sparse.csr.csr_matrix'>\n",
    "    print('-'*50)\n",
    "    print(res.toarray()) #转换成数组\n",
    "    print('-'*50)\n",
    "    # 逆转换，已经去除单个字母了\n",
    "    print(vector.inverse_transform(res))\n",
    "\n",
    "\n",
    "text2vec_en()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3 一段汉字文本如何数值化，对于汉字不能用空格来分割"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['python' '不用' '人生漫长' '人生苦短' '你们' '喜欢']\n",
      "--------------------------------------------------\n",
      "  (0, 3)\t1\n",
      "  (0, 5)\t1\n",
      "  (0, 0)\t2\n",
      "  (1, 0)\t1\n",
      "  (1, 2)\t1\n",
      "  (1, 4)\t1\n",
      "  (1, 1)\t1\n",
      "--------------------------------------------------\n",
      "[[2 0 0 1 0 1]\n",
      " [1 1 1 0 1 0]]\n",
      "--------------------------------------------------\n",
      "[array(['人生苦短', '喜欢', 'python'], dtype='<U6'), array(['python', '人生漫长', '你们', '不用'], dtype='<U6')]\n"
     ]
    }
   ],
   "source": [
    "# chinese_text---->number(vector)\n",
    "def text2vec_cn():\n",
    "    \"\"\"\n",
    "    对文本进行特征值化,单个汉字单个字母不统计，因为单个汉字字母没有意义\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "# NO1 实例化CountVectorizer\n",
    "    cv = CountVectorizer()\n",
    "# NO2 调用fit_transform输入并转换数据\n",
    "    chinese_text1 = \"人生苦短，我 喜欢 python python\"\n",
    "    chinese_text2 = \"人生漫长，你们，不用， python，哼\"\n",
    "    data = cv.fit_transform([chinese_text1, chinese_text2])\n",
    "\n",
    "    print(cv.get_feature_names_out())\n",
    "    print('-' * 50)\n",
    "    print(data)\n",
    "    print('-' * 50)\n",
    "    print(data.toarray())\n",
    "    print('-' * 50)\n",
    "    #单个汉字也去除了\n",
    "    print(cv.inverse_transform(data))\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "text2vec_cn()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 这段代码解释了中文text2vec_cn()中的原理和过程，以及分词的使用"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hanzivec\n",
      "今天 很 残酷 ， 明天 更 残酷 ， 后天 很 美好 ， 但 绝对 大部分 是 死 在 明天 晚上 ， 所以 每个 人 不要 放弃 今天 。\n",
      "--------------------------------------------------\n",
      "我们 看到 的 从 很 远 星系 来 的 光是在 几百万年 之前 发出 的 ， 这样 当 我们 看到 宇宙 时 ， 我们 是 在 看 它 的 过去 。\n",
      "--------------------------------------------------\n",
      "如果 只用 一种 方式 了解 某样 事物 ， 你 就 不会 真正 了解 它 。 了解 事物 真正 含义 的 秘密 取决于 如何 将 其 与 我们 所 了解 的 事物 相 联系 。\n",
      "--------------------------------------------------\n",
      "['一种' '不会' '不要' '之前' '了解' '事物' '今天' '光是在' '几百万年' '发出' '取决于' '只用' '后天' '含义'\n",
      " '大部分' '如何' '如果' '宇宙' '我们' '所以' '放弃' '方式' '明天' '星系' '晚上' '某样' '残酷' '每个'\n",
      " '看到' '真正' '秘密' '绝对' '美好' '联系' '过去' '这样']\n",
      "[[0 0 1 0 0 0 2 0 0 0 0 0 1 0 1 0 0 0 0 1 1 0 2 0 1 0 2 1 0 0 0 1 1 0 0 0]\n",
      " [0 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 1 3 0 0 0 0 1 0 0 0 0 2 0 0 0 0 0 1 1]\n",
      " [1 1 0 0 4 3 0 0 0 0 1 1 0 1 0 1 1 0 1 0 0 1 0 0 0 1 0 0 0 2 1 0 0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "#掌握如何对中文进行分词\n",
    "def cutword():\n",
    "    \"\"\"\n",
    "    通过jieba对中文进行分词\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    originText1=\"今天很残酷，明天更残酷，后天很美好，但绝对大部分是死在明天晚上，所以每个人不要放弃今天。\"\n",
    "\n",
    "    originText2=\"我们看到的从很远星系来的光是在几百万年之前发出的，这样当我们看到宇宙时，我们是在看它的过去。\"\n",
    "\n",
    "    originText3=\"如果只用一种方式了解某样事物，你就不会真正了解它。了解事物真正含义的秘密取决于如何将其与我们所了解的事物相联系。\"\n",
    "    con1 = jieba.cut( originText1)\n",
    "\n",
    "    con2 = jieba.cut( originText2)\n",
    "\n",
    "    con3 = jieba.cut(originText3)\n",
    "\n",
    "    # 转换成列表\n",
    "    # print(type(con1))\n",
    "    # print('-' * 50)\n",
    "    content1 = list(con1)\n",
    "    # print('-' * 50)\n",
    "    content2 = list(con2)\n",
    "    # print('-' * 50)\n",
    "    content3 = list(con3)\n",
    "    # print('-' * 50)\n",
    "    # print(content1)\n",
    "    # print('-' * 50)\n",
    "    # print(content2)\n",
    "    # print('-' * 50)\n",
    "    # print(content3)\n",
    "    # print('-' * 50)\n",
    "    # 把列表转换成字符串\n",
    "    c1 = ' '.join(content1)\n",
    "    c2 = ' '.join(content2)\n",
    "    c3 = ' '.join(content3)\n",
    "    # print('-' * 50)\n",
    "    # print(c1)\n",
    "    # print('-' * 50)\n",
    "    # print(c2)\n",
    "    # print('-' * 50)\n",
    "    # print(c3)\n",
    "    # print('-' * 50)\n",
    "\n",
    "\n",
    "    return c1, c2, c3\n",
    "\n",
    "\n",
    "def hanzivec():\n",
    "    \"\"\"\n",
    "    中文特征值化\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    c1, c2, c3 = cutword()\n",
    "    print('hanzivec')\n",
    "    print(c1)\n",
    "    print('-'*50)\n",
    "    print(c2)\n",
    "    print('-'*50)\n",
    "    print(c3)\n",
    "    print('-'*50)\n",
    "\n",
    "    cv = CountVectorizer()\n",
    "\n",
    "    data = cv.fit_transform([c1, c2, c3])\n",
    "\n",
    "    print(cv.get_feature_names_out())\n",
    "\n",
    "    print(data.toarray())\n",
    "    return None\n",
    "hanzivec()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "data": {
      "text/plain": "0.17609125905568124"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log10(1.5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "今天 很 残酷 ， 明天 更 残酷 ， 后天 很 美好 ， 但 绝对 大部分 是 死 在 明天 晚上 ， 所以 每个 人 不要 放弃 今天 。 我们 看到 的 从 很 远 星系 来 的 光是在 几百万年 之前 发出 的 ， 这样 当 我们 看到 宇宙 时 ， 我们 是 在 看 它 的 过去 。 如果 只用 一种 方式 了解 某样 事物 ， 你 就 不会 真正 了解 它 。 了解 事物 真正 含义 的 秘密 取决于 如何 将 其 与 我们 所 了解 的 事物 相 联系 。\n",
      "--------------------------------------------------\n",
      "<class 'list'>\n",
      "--------------------------------------------------\n",
      "['一种' '不会' '不要' '之前' '了解' '事物' '今天' '光是在' '几百万年' '发出' '取决于' '只用' '后天' '含义'\n",
      " '大部分' '如何' '如果' '宇宙' '我们' '所以' '放弃' '方式' '明天' '星系' '晚上' '某样' '残酷' '每个'\n",
      " '看到' '真正' '秘密' '绝对' '美好' '联系' '过去' '这样']\n",
      "--------------------------------------------------\n",
      "<class 'scipy.sparse._csr.csr_matrix'>\n",
      "--------------------------------------------------\n",
      "[[0.         0.         0.21821789 0.         0.         0.\n",
      "  0.43643578 0.         0.         0.         0.         0.\n",
      "  0.21821789 0.         0.21821789 0.         0.         0.\n",
      "  0.         0.21821789 0.21821789 0.         0.43643578 0.\n",
      "  0.21821789 0.         0.43643578 0.21821789 0.         0.\n",
      "  0.         0.21821789 0.21821789 0.         0.         0.        ]\n",
      " [0.         0.         0.         0.2410822  0.         0.\n",
      "  0.         0.2410822  0.2410822  0.2410822  0.         0.\n",
      "  0.         0.         0.         0.         0.         0.2410822\n",
      "  0.55004769 0.         0.         0.         0.         0.2410822\n",
      "  0.         0.         0.         0.         0.48216441 0.\n",
      "  0.         0.         0.         0.         0.2410822  0.2410822 ]\n",
      " [0.15698297 0.15698297 0.         0.         0.62793188 0.47094891\n",
      "  0.         0.         0.         0.         0.15698297 0.15698297\n",
      "  0.         0.15698297 0.         0.15698297 0.15698297 0.\n",
      "  0.1193896  0.         0.         0.15698297 0.         0.\n",
      "  0.         0.15698297 0.         0.         0.         0.31396594\n",
      "  0.15698297 0.         0.         0.15698297 0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# 规范{'l1'，'l2'}，默认='l2'\n",
    "# 每个输出行都有单位范数，或者：\n",
    "#\n",
    "# 'l2'：向量元素的平方和为 1。当应用 l2 范数时，两个向量之间的余弦相似度是它们的点积。\n",
    "#\n",
    "# 'l1'：向量元素的绝对值之和为 1。参见preprocessing.normalize。\n",
    "\n",
    "\n",
    "# 通过在文档频率上加一来平滑 idf 权重，就好像看到一个额外的文档包含集合中的每个术语恰好一次。防止零分裂。\n",
    "# 比如训练集中有某个词，测试集中没有，就是生僻词，就会造成n(x)分母为零，log(n/n(x)),从而出现零分裂\n",
    "\n",
    "def tfidfvec():\n",
    "    \"\"\"\n",
    "    中文特征值化,计算tfidf值\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    c1, c2, c3 = cutword()\n",
    "\n",
    "    print(c1, c2, c3)\n",
    "    print('-'*50)\n",
    "    print(type([c1, c2, c3]))\n",
    "    print('-'*50)\n",
    "\n",
    "    # smooth_idf=False,不平滑，就是不加1，True，平滑就是加1，防止零分裂，\n",
    "    # 比如训练集中有某个词，测试集中没有，就是生僻词，就会造成n(x)分母为零，log(n/n(x)),从而出现零分裂\n",
    "    tf = TfidfVectorizer(smooth_idf=True)\n",
    "\n",
    "    data = tf.fit_transform([c1, c2, c3])\n",
    "\n",
    "    print(tf.get_feature_names_out())\n",
    "    print('-'*50)\n",
    "    print(type(data))\n",
    "    print('-'*50)\n",
    "    print(data.toarray())\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "tfidfvec()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4 特征处理，不同的特征拉到到同一个量纲"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 归一化\n",
    "#### 归一化公式是：(X-Xmin)/(Xmax-Xmin)\n",
    "#### 假设 nums1=[90, 2, 10, 40] Xmin=2 Xmax=90\n",
    "#### (90-2)/(90-2)=88/88=1\n",
    "#### (2-2)/(90-2)=0/88=0\n",
    "#### (10-2)/(90-2)=8/88=0.09\n",
    "#### (40-2)/(90-2)=38/88=0.43"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.         -1.         -1.         -1.        ]\n",
      " [-1.          1.          1.          0.66666667]\n",
      " [ 0.          0.          0.2         1.        ]]\n"
     ]
    }
   ],
   "source": [
    "def mm():\n",
    "    \"\"\"\n",
    "    归一化处理\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    # 归一化缺点 容易受极值的影响\n",
    "    #feature_range代表特征值范围，一般设置为(-1,1),默认是(0,1)\n",
    "    mm = MinMaxScaler(feature_range=(-1,1))#这里是()元组\n",
    "    nums1=[90, 2, 10, 40]\n",
    "    nums2=[60, 4, 15, 45]\n",
    "    nums3=[75, 3, 13, 46]\n",
    "    #transform和fit_transform不同是，transform用于测试集，而且不会重新找最小值和最大值\n",
    "    data= mm.fit_transform([nums1,nums2,nums3])\n",
    "    print(data)\n",
    "\n",
    "\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "mm()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": "-1.068779614944516"
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.square(1-2.333)+np.square(2-2.333)+np.square(4-2.333))/3\n",
    "print('-'*50)\n",
    "(1-2.333)/np.sqrt(1.55555)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 标准化\n",
    "#### 标准化公式是：(X-Xmean)/S\n",
    "#### S是标准差，Xmean是平均值\n",
    "#### 标准差的公式是：S=sqrt((平方差和)/n)\n",
    "#### 标准差是方差的算术平方根，标准差能反映一个数据集的离散程度\n",
    "#### 假设 nums1=[90, 2, 10, 40] Xmean=35.5 S=35.5"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def stand():\n",
    "    \"\"\"\n",
    "    标准化缩放，不是标准正太分布，只均值为0，方差为1的分布\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    std = StandardScaler()\n",
    "\n",
    "    data = std.fit_transform([[1., -1., 3.], [2., 4., 2.], [4., 6., -1.]])\n",
    "\n",
    "    print(data)\n",
    "    print('-' * 50)\n",
    "    print(std.mean_)\n",
    "    print('-' * 50)\n",
    "    print(std.var_)\n",
    "    print(std.n_samples_seen_)  # 样本数\n",
    "    print('-' * 50)\n",
    "    #为了证明上面输出的结果的均值是为0的，方差为1\n",
    "    data1 = std.fit_transform([[-1.06904497, -1.35873244, 0.98058068],\n",
    "                               [-0.26726124, 0.33968311, 0.39223227],\n",
    "                               [1.33630621, 1.01904933, -1.37281295]])\n",
    "    # print(data1)  #这个并不是我们想看的，没意义\n",
    "    # 均值\n",
    "    print(std.mean_)\n",
    "    # 方差\n",
    "    print(std.var_)\n",
    "    # 样本数\n",
    "    print(std.n_samples_seen_)\n",
    "    return None\n",
    "\n",
    "\n",
    "stand()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5 缺失值处理"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3. 2.]\n",
      " [3. 3.]\n",
      " [7. 3.]\n",
      " [4. 3.]]\n"
     ]
    }
   ],
   "source": [
    "#下面是填补，针对删除，可以用pd和np\n",
    "def im():\n",
    "    \"\"\"\n",
    "    缺失值处理\n",
    "    :return:NOne\n",
    "    \"\"\"\n",
    "    # NaN, nan,缺失值必须是这种格式,如果是np.nan,就不用管了\n",
    "    # 如果是？号(或者其他符号)，就要replace换成np.nan\n",
    "    # strategy='mean'是用均值填补，还有'median'中位数，'most_frequent'众数\n",
    "    im = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "\n",
    "    data = im.fit_transform([[3, 2], [np.nan, 3], [7,3], [4, 3]])\n",
    "\n",
    "    print(data)\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "im()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6 降维\n",
    "#### 降维就是特征数变少\n",
    "#### 降维可以提高模型训练速度（模型的参数减少了）"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [4]\n",
      " [1]]\n",
      "The surport is [2]\n"
     ]
    }
   ],
   "source": [
    "def var():\n",
    "    \"\"\"\n",
    "    特征选择-删除低方差的特征\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    #默认只删除方差为0,threshold是方差阈值，删除比这个值小的那些特征\n",
    "    var = VarianceThreshold(threshold=0.9)\n",
    "\n",
    "    data = var.fit_transform([[0, 2, 0, 3],\n",
    "                              [0, 1, 4, 3],\n",
    "                              [0, 1, 1, 3]])\n",
    "\n",
    "    print(data)\n",
    "    # 获得剩余的特征的列编号\n",
    "    print('The surport is %s' % var.get_support(True))\n",
    "    return None\n",
    "\n",
    "\n",
    "var()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.13587302e-16  3.82970843e+00]\n",
      " [-5.74456265e+00 -1.91485422e+00]\n",
      " [ 5.74456265e+00 -1.91485422e+00]]\n"
     ]
    }
   ],
   "source": [
    "def pca():\n",
    "    \"\"\"\n",
    "    主成分分析进行特征降维\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    # n_ components:小数 0~1 90% 业界选择 90~95%\n",
    "    #\n",
    "    # 如果是整数   减少到的特征数量\n",
    "\n",
    "    pca = PCA(n_components=0.9)\n",
    "\n",
    "    data = pca.fit_transform([[2, 8, 4, 5], [6, 3, 0, 8], [5, 4, 9, 1]])\n",
    "\n",
    "    print(data)\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "pca()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}